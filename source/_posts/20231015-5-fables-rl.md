---
title: 5 Fables of Reinforcement Learning | 强化学习寓言五则
date: 2023-10-15 00:00:00
---

笔者最近入门强化学习，阅读 [*Grokking Deep Reinforcement Learning*](https://book.douban.com/subject/35238587/) 体验很好，作者经常会给一个“Miguel’s Analogy” 告诉读者这个概念在生活中的类比。

强化学习确实就是这样一个有很多生活类比的学科，虽然理论中充满了各种复杂的数学公式，但其实很多概念都可以从生活直觉中获得。我们日常生活里，不断地学新东西，比如像玩游戏或学骑车一样，就和强化学习很像：就是个体（比如我们或机器人）与环境互动，找最好的方法来做决策，争取最大的奖励。

强化学习真正讨论的是让智能体“如何学”。有些方法是从自己的经验中学，有些是看别人怎么做。有的追求长久的收益，有的想要短期的快乐。

这篇博客要介绍强化学习的核心概念，用五组概念对比帮助理解。每组概念对比都有个故事和图解，让大家更容易明白这些深奥的概念。

本文中大量文本和图片都是GPT-4生成的。

# 1. 贪婪Greedy vs 随机Random

## 1.1 寓言《湖中的渔夫》

![Untitled](/images/fable5_(1).jpg)

在远古的时候，宁静的月光村旁边有一片被雾气笼罩的深邃湖泊，名叫梦之湖。村中的老人们流传着关于湖中蕴藏的各种神奇鱼类的故事，但大多数人都只曾遇到其中的几种。

湖边常年驻扎着两位渔夫——尼尔和利亚姆，他们每天与湖水亲密接触。

**尼尔**沉浸于古老的传统之中。他的鱼竿总是投向那片熟悉而又鱼群聚集的湖区，那里曾无数次为他带来满满的收获。在他眼中，那片湖水是黄金地段，无需更改。

而**利亚姆**是个勇敢的冒险者。他相信梦之湖的每一片水域都有可能藏有奇遇。每天，他的鱼竿都在不同的水域探索。有时，他找到了村中传说中的那些珍稀鱼类，令人羡慕。但有时，太阳西沉时，他的鱼篓仍是空空如也。

日子过去，尼尔那片“黄金地段”里的鱼群逐渐稀少，他渐感迷茫。而利亚姆，虽然曾多次空手回家，但他已经在梦之湖中找到了几处隐藏的宝藏之地。

这个故事告诉我们，在生活中，尼尔般的**贪婪策略**可能安稳，但过度的依赖会使我们失去新的机会。而利亚姆的**随机策略**虽然充满未知，但它让人发现了更多的可能。真正的智慧在于找到两者之间的平衡。

## 1.2 概念解释

在强化学习中，探索策略决定了智能体如何在学习过程中选择动作。这是一个核心问题，因为智能体需要在探索未知的动作（为了发现可能的最佳策略）和利用已知的信息（为了获得即时的奖励）之间找到平衡。其中，**greedy**（贪婪）策略和**random**（随机）策略是两种典型的方法。

1. **Greedy（贪婪）策略**:
    - 在每一步，智能体选择使其预期奖励最大化的动作。
    - 它完全基于当前已知的信息来做决策，不考虑未知的可能性。
    - 优点是它马上能得到奖励，但缺点是它可能错过更好的长期奖励，因为没有进行足够的探索。
2. **Random（随机）策略**:
    - 在每一步，智能体随机选择一个可能的动作，不考虑任何以前的经验或预期奖励。
    - 这样可以确保智能体充分探索所有可能的动作。
    - 优点是它可以进行广泛的探索，但缺点是它可能经常选择次优或劣质的动作。

为了在两者之间找到平衡，很多算法采用了ε-greedy策略，其中有一个参数ε（通常是一个较小的值）来确定智能体以多大的概率选择随机动作，而余下的时间则选择贪婪动作。这样，在大部分时间内，智能体会根据已知的信息做出最佳决策，但偶尔也会随机探索新的动作。

# 2. 策略梯度Policy Gradient vs 基于值的方法Value-Based Method

## 2.1 寓言故事 《指南者和宝藏猎人》

![Untitled](/images/fable5_(2).jpg)

在神秘的云梦大陆，传说中有一处被时光封印的宝藏——时之钥匙，隐藏于一座迷雾之中的古老迷宫。寻宝者们都知道，谁能找到这把钥匙，就能掌握时间的秘密。

在无数的冒险家中，**雷诺**和**伊尔文**最为出名。他们都对那把传说中的钥匙心驰神往，决定展开寻宝之旅。

**雷诺**是一名拥有独特直觉的旅者。他相信，迷宫中蕴藏的是一种古老的力量，可以与之沟通。每当他深入迷宫，他都试图与迷宫的心灵产生共鸣，凭感觉来选择方向。随着他不断的探索，他逐渐学会了如何与迷宫的灵魂对话，寻找正确的路。

而**伊尔文**则是一名计划高手。他手中的羊皮地图记录了每次冒险的轨迹和选择的结果。每当他来到一个分岔路口，他都会回想起之前在这里的选择，并在地图上做标记。在他的地图上，某些路线被标记为有希望，而某些则是陷阱。每次进入迷宫，他都基于地图的指引和少许的直觉来决策。

时光荏苒，两位冒险家在迷宫中留下了自己的传奇。雷诺凭直觉绘制了一条未知的道路，虽然时常会误入歧途，但他的冒险之路充满了意外的惊喜。伊尔文则依赖着他的地图，每一步都是历经思考和判断的选择。

这个故事揭示了策略梯度与Q-learning在冒险之路上的区别。雷诺(**策略梯度**)展现了直观并不断优化策略的旅程，而伊尔文(**基于值的方法**)则是以经验和累积的知识为导向，寻找最佳的行动方案。而在现实的探索中，选择哪条路，全凭冒险者的内心召唤。

## 2.2 概念解释

强化学习中的策略梯度方法（Policy Gradient）和基于值的方法（如Q-learning）是两种主要的学习策略，它们在学习过程和优化目标上有所不同。下面是它们之间的主要区别：

1. **策略梯度（Policy Gradient）**
    1. 直接对策略进行优化。策略通常表示为状态到动作的映射。输出的是一个动作或一个动作的概率分布。
    2. 由于策略可以是随机的（如softmax策略），探索可以直接从策略中得到。
2. **基于值的方法（Value-Based）**
    1. 输出的是每个动作的预期回报，优化的是值函数（如Q函数），并间接地通过这些值来确定最佳策略。
    2. 通常需要额外的策略，如ε-greedy，来确保探索。

从人格来比喻，策略梯度倾向于“直觉”，而基于值的方法倾向于“经验”。

当前SOTA的actor-critic算法，结合了这两种方法。其中，actor是策略梯度，倾向于直觉，而critic是一个值方法，倾向于经验。

# 3. 蒙特卡洛MC vs 时间差分TD

## 3.1 寓言 《果园的收获》

![Untitled](/images/fable5_(3).jpg)

在古老的蜜桃村，传说中有两位果农大师：**塞洛斯**与**达米恩**。他们的果园坐落在村的东西两侧，每到秋天，便成为村中的焦点。

**塞洛斯**是一个守旧的果农。他坚信，真正的果实只有在秋天结束，经过整个季节的滋养后，才会展现出它们的真实价值。因此，他总是等到秋季的最后一天，当所有果实都完全成熟，才开始大规模地收获。此后，他会坐在果园的中央，眼望着那片金黄，总结哪棵树在这个季节中真正做得出色。

而**达米恩**，则是一名新派果农。他每天清晨都会穿越果园，亲自挑选那些当天最为成熟的果实。他认为，要想真正了解一棵树，就需要观察它的每一个阶段。每天的小规模收获不仅给了他即时的满足，还让他对每棵树在不同时段的表现有了深入的了解。

当秋天的尾声即将落下，两位果农都获得了自己期待的回报。塞洛斯看到了一个季节的整体表现，知道哪些树经过了四季的考验真正繁盛。而达米恩，通过他日复一日的努力，知道了哪棵树在哪一个月、哪一周、甚至哪一天最为灿烂。

这个故事诉说了两种不同的观察与回馈策略。有些时候，我们需要全面而远景的观察（如塞洛斯的蒙特卡洛策略），而有时，我们需要关注细节并持续地反馈（如达米恩的时间差分策略）。在生活的不同阶段，选择哪种策略，全看心之所向。

## 2.2 概念解释

上文说到的**基于值的方法**都需要对策略进行评估(值函数估计)，这里就是两种截然不同的评估方法。

蒙特卡洛方法（Monte Carlo，简称MC）和时间差分方法（Temporal Difference，简称TD）在学习过程中使用的数据和更新方式有所不同。以下是它们的主要区别：

1. **蒙特卡洛 (MC)**
    1. MC方法是基于完整的回合（episode）进行估计。只有在一个回合结束后，我们才知道从某状态出发获得的实际回报。使用这个完整的回报来估计值函数。
    2. 只在回合结束后进行更新，因为只有到那时，我们才有完整的回报来进行值函数的估计。
    3. MC方法具有较高的方差和较低的偏差，因为它是基于实际经验（完整的回合回报）进行估计。
    4. 可能需要更多的回合来稳定地学习，因为它是基于完整的回合进行更新的。
2. **时间差分 (TD)**
    1. TD方法是基于单步的立即回报和后续状态的估计值进行估计。它结合了实际获得的立即回报和对未来回报的预测来估计值函数。
    2. 在每一步后都可以进行更新，因为它使用了对未来的预测。
    3. TD方法具有较低的方差和较高的偏差，因为它是基于对未来的预测进行估计。
    4. 可能学习得更快，因为它在每一步都可以进行更新。

总的来说，MC和TD方法在强化学习中有其各自的优势和应用场景。有时，它们也可以结合使用，如TD(λ)方法，它结合了TD和MC的特点，通过一个参数λ来平衡它们的权重。

# 4. On-policy vs Off-policy

## 4.1 寓言《烹饪大赛》

![Untitled](/images/fable5_(4).jpg)

在宁静的梅洛小镇，传闻有两位大厨：**艾米莉亚**与**奥特尔**。他们均决定参与一场盛大的梅洛烹饪大赛，大赛的标签是“创意之风”，要求大厨们呈现出味觉的魔法与烹饪的新思维。

**艾米莉亚**，小镇的新星，她的厨艺之旅遵循“今日所学，明日所用”的原则。每天，她为亲朋好友烹制新颖的佳肴，并凭借他们的第一反应进行调整。每次的调炅，都融入了前一天的新发现与反馈。尽管这种方式使她的食材和精力消耗巨大，但她相信这是与时俱进的代价。

与此同时，**奥特尔**，梅洛的资深厨师，选择了与众不同的道路。虽然他每天也会尝试新的创作，但他并不完全依赖每日的反馈。在他的私人书房，放置着一本厚厚的“美食之旅”，记录着他长年累月的探索和反思。每晚，他都会重温其中几页，借此与过去的自己对话，汲取经验。他相信，真正的智慧来自于对过去的回顾和思考。

当大赛的钟声敲响，两位大厨各展神通。艾米莉亚呈现的菜肴如同一幅色彩斑斓的画，充满了活力和新颖感，每一道都像是昨天的改良版。而奥特尔所献上的，是经典与创新的完美交融，每道菜都仿佛诉说着一个时代的故事。

评审团为艾米莉亚(on-policy)的创新勇气和奥特尔(off-policy)的匠心独运点赞。两位大厨都受到了高度的赞誉，但他们的策略与展现的技巧截然不同。

这个故事揭示了现实中的两种学习路径：一是紧跟时代的脚步，即时反馈；另一是深入沉思，累积经验。人们在人生的道路上，根据自己的目标和环境，选择适合自己的学习方式。

## 4.2 概念解释

上文提到的值方法中，评估策略时可以使用不同的经验方式。

**on-policy** 和 **off-policy** 是两种不同的评估经验的方式，他们的主要区别：

1. **On-policy**:
    - 在 on-policy 学习中，智能体根据当前的策略同时进行探索和学习。
    - 智能体学习和执行同一个策略。也就是说，它根据当前策略采取动作，并根据这些经验更新策略。
    - SARSA（State-Action-Reward-State-Action）是一个典型的 on-policy 学习算法。
    - 由于它们同时进行学习和执行，on-policy 方法可能需要更多的样本来稳定地学习，尤其是在策略空间很大时。
2. **Off-policy**:
    - 在 off-policy 学习中，智能体可以根据不同于执行策略的策略来学习。
    - 智能体可以根据一种策略采取动作（探索策略）并根据另一种策略学习（目标策略）。这使得智能体可以从过去的经验中学习，甚至可以从其他智能体的经验中学习。
    - Q-learning 是一个典型的 off-policy 学习算法。DQN（Deep Q-Network）也是一个使用经验回放的 off-policy 方法。
    - 由于智能体可以从旧的经验中反复学习，off-policy 方法可以更高效地利用数据。这也使得它们可以利用像经验回放这样的技术，从一个经验回放缓冲区中随机抽取样本进行学习，从而打破数据之间的时间相关性。

总结起来，选择 on-policy 和 off-policy 的决策取决于任务、数据效率、实时性需求和学习稳定性等因素。在某些任务中，实时学习和反应是关键（例如，实时游戏），这时 on-policy 方法可能更合适。而在其他任务中，如训练一个机器人，数据效率和学习稳定性可能更重要，这时 off-policy 方法，特别是那些结合经验回放的方法，可能会更有优势。

# 5. Accumulating Trace vs Replacing Trace

## 5.1 寓言故事：《寻宝冒险》

![Untitled](/images/fable5_(5).jpg)

在一个叫做塞雷斯提亚的古老国度，坐落着一座被时光遗忘的迷宫，名为“遗失之塔”。传说其中隐藏着一颗能够赋予持有者无尽智慧的水晶心。吸引了无数英雄前来挑战，但所有人都消失在了迷宫的深处。然而，两位英勇的探险家——**亚尔文**和**布兰卡**，勇敢地踏上了这次寻宝之旅。

亚尔文进入迷宫时，带着一本古老的羊皮卷笔记本。每次他在蜿蜒的迷宫中找到一个神秘的线索或古老的符文，他都会仔细地在羊皮卷上勾画下来。当他反复探索同一个地方并不断发现线索时，他的笔记就会变得越来越丰富。他坚信，所有的线索最终都会指引他找到水晶心。

而布兰卡则有她独特的方式。她手中的笔记本上，每一个地方只有一个记号。每次在相同的地方发现新的线索，她就会擦去旧的，仅仅保留最新的那一个。她相信，这样可以让她始终保持敏锐的直觉，不被过去的线索所束缚。

“遗失之塔”内部布满了机关，转动的楼梯和易变的房间。依赖他的羊皮卷，亚尔文走得相对稳定，每个地方都经过了反复的验证。布兰卡则经常改变路线，有时因为她的直觉找到了隐藏的通道，有时则陷入困境。

当两位英雄最终站在水晶心的面前，他们的旅程截然不同，但他们都到达了终点。亚尔文(accumulation trace)沿着一个经过深思熟虑和验证的路线，步步为营；而布兰卡(replacing trace)则像风一样，不断适应和变化，有时冒险，有时收获。

这个传说教会了塞雷斯提亚的人们，无论是积累的经验还是直觉的指引，选择哪一条道路都取决于每个人的性格和目标。

## 5.2 概念解释

在上文 off-policy 类算法中，需要积累过去的经验（称为trace，迹），而如何积累过去的经验是一个问题，**accumulating trace** 和 **replacing trace** 是两种不同的 trace 更新策略。

1. **Accumulating Trace**:
    - 这种方法是对每个访问过的状态-动作对的 trace 值进行累加。
    - 当智能体在时间 t 访问状态 *s* 并采取动作 *a* 时，其对应的 trace *e*(*s*,*a*) 会增加 1。
    - 所有其他的 trace 会乘以一个衰减因子 *γλ*，其中 *γ* 是折扣因子，*λ* 是 trace 的衰减参数。
    - 公式表示： *e*(*s*,*a*)=*γλe*(*s*,*a*)+1
2. **Replacing Trace**:
    - 对于替换 trace，我们不再累加 trace 值。相反，每次访问状态-动作对，其对应的 trace 值都会被设置为 1。
    - 这意味着，无论智能体之前访问了多少次该状态-动作对，该位置的 trace 都会被“替换”为 1。
    - 公式表示：如果智能体在时间 t 访问了状态 *s* 并采取了动作 *a*，则 *e*(*s*,*a*)=1；对于所有其他状态-动作对，其 trace 仍然乘以 *γλ*。

# 6. 总结

强化学习中，智能体在和环境的”交互-评估-提升-再交互”的循环中提升认知。

智能体在交互阶段，输入环境状态（state），通过策略（policy）的计算，获得动作（action）。这里策略的学习过程，会有 random vs greedy 的选择问题，而通常混合策略更加有效。在评估阶段，有MC vs TD的不同经验评估节奏。在提升阶段，有 on-policy 和 off-policy 的不同经验选择方式，有 accumulating trace 和 replacing trace 的不同经验积累方式。

智能体在整个循环里，还有 policy-gradient 和 value-base method 类不同的模式偏好，及它们混合的 actor-critic 类方法。

本文并不力求帮助读者理解强化学习的全貌和算法，而是希望帮读者更直觉地理解强化学习中的概念。

本文中大量文本和图片都是GPT-4生成的，虽然GPT可以在细节概念的阐释上可以做得很好，但对于概念的宏观图景，还是这本 *Grokking deep reinforcement learning* 勾画地更为清晰。本文的创作中，有几组概念的对比直接来源于此书的注释，也有几组来自于笔者自己的的体会。笔者体会到GPT作为copilot创作的便利性，但还需要一些人工指引。

# 参考资料

Morales, Miguel. *Grokking deep reinforcement learning*. Manning Publications, 2020.